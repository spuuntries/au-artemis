{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize elevant directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#main ARTEMIS directory\n",
    "artemis_repo_dir = os.environ[\"ARTEMIS_DIR\"]\n",
    "\n",
    "# where is the sample_speaker.py script?\n",
    "sample_speaker_script = os.path.join(artemis_repo_dir, \"artemis\",\"artemis\", \"scripts\", \"sample_speaker.py\")\n",
    "\n",
    "#path to ArtEmis/COCO preprocessed data\n",
    "preprocessed_data_dir = os.path.join(artemis_repo_dir, \"preprocessed_artemis\")\n",
    "\n",
    "#where is the pretrained model?\n",
    "model_path = os.path.join(artemis_repo_dir, \"pretrained_models\", \"emo_grounded_model\", \"checkpoints\",\"best_model.pt\")\n",
    "\n",
    "\n",
    "#parent folder for this script\n",
    "custom_caption_dir = os.path.join(artemis_repo_dir, \"custom_scripts\",\"emo_caption_custom_images\")\n",
    "\n",
    "\n",
    "# this file will be input for the captioner\n",
    "custom_img_csv = os.path.join(custom_caption_dir,\"temporary_files\",\"custom_images.csv\")\n",
    "\n",
    "# where are the images to caption?\n",
    "img_dir= os.path.join(custom_caption_dir, \"images\")\n",
    "\n",
    "# where to pickle the results?\n",
    "pickle_out_file = os.path.join(custom_caption_dir,\"temporary_files\",\"pickled_data.pkl\")\n",
    "\n",
    "#for logs\n",
    "log_dir = os.path.join(custom_caption_dir, \"temporary_files\",\"logs\")\n",
    "\n",
    "#configuration file for sample_speaker.py\n",
    "configuration_file_path = os.path.join(artemis_repo_dir, \"pretrained_models\",\"emo_grounded_model\",\"config.json.txt\")\n",
    "\n",
    "#final captions will be here:\n",
    "csv_out_file = os.path.join(custom_caption_dir, \"outputs\",\"custom_captions.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the CSV for custom images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sample_speaker.py` script needs a csv of one column with header `image_file` and whose rows are the absolute paths of the images to be captioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get image paths\n",
    "filenames = [os.path.join(img_dir, filename) for filename in os.listdir(img_dir) if os.path.isfile(os.path.join(img_dir, filename))]\n",
    "filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the CSV\n",
    "with open(custom_img_csv, 'w') as custom_csv:\n",
    "    custom_csv.write('image_file') #header\n",
    "    custom_csv.write('\\n')\n",
    "    #file paths:\n",
    "    for filename in filenames:\n",
    "        custom_csv.write(filename)\n",
    "        custom_csv.write('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit configuration file\n",
    "\n",
    "This file is in ARTEMIS_DIR/pretrained_models/emo_grounded_model/config.json.txt and it is a JSON file with parameters for the captioning algorithm. \n",
    "I think the program should be modified to accept them in the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(configuration_file_path) as configuration_file:\n",
    "    configuration_json = json.load(configuration_file)\n",
    "\n",
    "configuration_json[\"data_dir\"] = preprocessed_data_dir\n",
    "configuration_json[\"log_dir\"] = log_dir\n",
    "\n",
    "with open(configuration_file_path, \"w\") as configuration_file:\n",
    "    json.dump(configuration_json, configuration_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/lvasquezreina/supproj/repo/M1-supervised_project/ARTEMIS/artemis/artemis/scripts/sample_speaker.py\", line 32, in <module>\n",
      "    speaker, epoch, data_loaders = load_saved_speaker(args.speaker_saved_args, args.speaker_checkpoint,\n",
      "  File \"/home/lvasquezreina/supproj/artemis/artemis/artemis/in_out/neural_net_oriented.py\", line 279, in load_saved_speaker\n",
      "    epoch = load_state_dicts(model_ckp, model=model, map_location='cpu')\n",
      "  File \"/home/lvasquezreina/supproj/artemis/artemis/artemis/in_out/neural_net_oriented.py\", line 230, in load_state_dicts\n",
      "    value.load_state_dict(checkpoint[key])\n",
      "  File \"/home/lvasquezreina/supproj/artemis/arte-venv/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1482, in load_state_dict\n",
      "    raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
      "RuntimeError: Error(s) in loading state_dict for ModuleDict:\n",
      "\tsize mismatch for decoder.word_embedding.weight: copying a param with shape torch.Size([14469, 128]) from checkpoint, the shape in current model is torch.Size([35467, 128]).\n",
      "\tsize mismatch for decoder.next_word.weight: copying a param with shape torch.Size([14469, 512]) from checkpoint, the shape in current model is torch.Size([35467, 512]).\n",
      "\tsize mismatch for decoder.next_word.bias: copying a param with shape torch.Size([14469]) from checkpoint, the shape in current model is torch.Size([35467]).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "process = subprocess.run(\n",
    "    [\n",
    "    \"python3\",\n",
    "    sample_speaker_script,\n",
    "    \"-speaker-saved-args\",\n",
    "    configuration_file_path,\n",
    "    \"-speaker-checkpoint\",\n",
    "    model_path,\n",
    "    #\"-data-dir\",\n",
    "    #preprocessed_data_dir,\n",
    "    \"-img-dir\" ,\n",
    "    img_dir,\n",
    "    \"-out-file\" ,\n",
    "    pickle_out_file,\n",
    "    #'-log-dir',\n",
    "    #log_dir,\n",
    "    \"--custom-data-csv\",\n",
    "    custom_img_csv\n",
    "    ],\n",
    "    stdout=subprocess.PIPE\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process pickled output into CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "#from IPython.display import display\n",
    "#from PIL import Image\n",
    "\n",
    "df = pd.read_pickle(pickle_out_file)\n",
    "df.to_csv(csv_out_file)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "832dc477f37f3066da477dde10711e0efb7941b501ae5ab3032353d70c19ef2e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 ('arte-venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
